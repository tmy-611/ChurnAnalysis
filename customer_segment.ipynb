{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel: Python 3.10.5\n",
    "import os\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.window import Window\n",
    "from functools import reduce\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning + Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional if not already set in system env vars\n",
    "os.environ[\"JAVA_HOME\"] = \"C:\\\\Program Files\\\\Java\\\\jdk-1.8\"\n",
    "os.environ[\"SPARK_HOME\"] = \"C:\\\\spark-3.5.1-bin-hadoop3\"\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataCleaning\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"./data/OnlineRetail.csv\", header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Data Profiling, Anomaly Detection and Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 Check the number of colummns and rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns:  8\n",
      "Number of rows:  541909\n"
     ]
    }
   ],
   "source": [
    "# Check the number of columns and rows\n",
    "print(\"Number of columns: \", len(df.columns))\n",
    "print(\"Number of rows: \", df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 Abnormal 1: Check and correct Data Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert datatype of InvoiceDate to timestamp\n",
    "df_cleaned = df.withColumn(\"InvoiceDate\", to_timestamp(\"InvoiceDate\", \"M/d/yyyy H:mm\"))\n",
    "\n",
    "# Re-check schema\n",
    "df_cleaned.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3 Abnormal 2: Check and handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|        0|        0|       1454|       0|          0|        0|    135080|      0|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in original data\n",
    "missing_values = df_cleaned.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) for c in df_cleaned.columns\n",
    "])\n",
    "missing_values.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Description: 1,454 --> remove since it is hard to guess the product description\n",
    "- CustomerID: 135,080 --> might keep them, change `null` value into `Unknown`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the null values in Description\n",
    "df_cleaned = df_cleaned.filter(df_cleaned.Description.isNotNull())\n",
    "\n",
    "# Change the value in CustomerID into \"Unknown\" for null values\n",
    "df_cleaned = df_cleaned.withColumn(\"CustomerID\", when(df_cleaned.CustomerID.isNull(), \"Unknown\").otherwise(df_cleaned.CustomerID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|        0|        0|          0|       0|          0|        0|         0|      0|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Re-check for missing values in the cleaned data\n",
    "missing_values = df_cleaned.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) for c in df_cleaned.columns\n",
    "])\n",
    "missing_values.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.4 Abnormal 3: Check and handle duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows:  5268\n"
     ]
    }
   ],
   "source": [
    "# Check the number of duplicate rows\n",
    "duplicate_count = df_cleaned.count() - df_cleaned.dropDuplicates().count()\n",
    "print(\"Number of duplicate rows: \", duplicate_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows\n",
    "df_cleaned = df_cleaned.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows:  0\n"
     ]
    }
   ],
   "source": [
    "# Re-check the number of duplicate rows\n",
    "duplicate_count = df_cleaned.count() - df_cleaned.dropDuplicates().count()\n",
    "print(\"Number of duplicate rows: \", duplicate_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5 Abnormal 4: Handle negative Quantity and UnitPrice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the number of cancelled order:** The InvoiceNo starts with C (has negative Quantity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of canceled orders:  9251\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|  C536825|    22617|BAKING SET SPACEB...|      -1|2010-12-02 17:27:00|     4.95|     15384|United Kingdom|\n",
      "|  C537251|    22747|POPPY'S PLAYHOUSE...|      -6|2010-12-06 10:45:00|      2.1|   Unknown|United Kingdom|\n",
      "|  C537805|    22197|SMALL POPCORN HOLDER|      -1|2010-12-08 13:18:00|     0.72|     15311|United Kingdom|\n",
      "|  C538103|    22941|CHRISTMAS LIGHTS ...|      -2|2010-12-09 15:13:00|      8.5|     17442|United Kingdom|\n",
      "|  C538768|    84378|SET OF 3 HEART CO...|     -24|2010-12-14 11:34:00|     1.25|     14829|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cancel = df_cleaned.filter(col(\"InvoiceNo\").startswith(\"C\"))\n",
    "print(\"Number of canceled orders: \", cancel.count())\n",
    "cancel.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check for invalid values in Quantity and UnitPrice:**\n",
    "- Quantity < 0\n",
    "- UnitPrice < 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negative Quantity:  9725\n",
      "+---------+--------+\n",
      "|InvoiceNo|Quantity|\n",
      "+---------+--------+\n",
      "|  C536825|      -1|\n",
      "|  C537251|      -6|\n",
      "|  C537805|      -1|\n",
      "|  C538103|      -2|\n",
      "|  C538768|     -24|\n",
      "+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quantity has negative value\n",
    "negative_Quantity = df_cleaned.filter(col(\"Quantity\") < 0).count()\n",
    "print(\"Number of negative Quantity: \", negative_Quantity)\n",
    "df_cleaned.filter(col(\"Quantity\") < 0).select(\"InvoiceNo\", \"Quantity\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negative UnitPrice:  2\n",
      "+---------+---------+\n",
      "|InvoiceNo|UnitPrice|\n",
      "+---------+---------+\n",
      "|  A563186|-11062.06|\n",
      "|  A563187|-11062.06|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# UnitPrice has negative value\n",
    "negative_UnitPrice = df_cleaned.filter(col(\"UnitPrice\") < 0).count()\n",
    "print(\"Number of negative UnitPrice: \", negative_UnitPrice)\n",
    "df_cleaned.filter(col(\"UnitPrice\") < 0).select(\"InvoiceNo\", \"UnitPrice\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that cancelled orders also have the negative Quantity.\n",
    "- The dataset contains 9,251 cancelled orders, while 9,725 orders have a negative quantity.\n",
    "- Therefore, in the Data Cleaning section, when we remove orders with negative quantities, we also remove the cancelled orders at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove abnormal Quantity and UnitPrice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Quantity count: 0\n",
      "Negative UnitPrice count: 0\n",
      "Number of cancelled orders:  0\n"
     ]
    }
   ],
   "source": [
    "# Filter out negative Quantity and UnitPrice\n",
    "df_cleaned = df_cleaned.filter((col(\"Quantity\") > 0) & (col(\"UnitPrice\") > 0))\n",
    "\n",
    "# Check negative Quantity and UnitPrice again\n",
    "print(\"Negative Quantity count:\", df_cleaned.filter(col(\"Quantity\") < 0).count())\n",
    "print(\"Negative UnitPrice count:\", df_cleaned.filter(col(\"UnitPrice\") < 0).count())\n",
    "\n",
    "# Check number of cancelled orders again\n",
    "cancel = df_cleaned.filter(col(\"InvoiceNo\").startswith(\"C\"))\n",
    "print(\"Number of cancelled orders: \", cancel.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.6 Abnormal 5: Identify abnormal `StockCode`-`Description` pairs that are not actual products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check abnormal StockCode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------------------------+\n",
      "|StockCode   |Description                       |\n",
      "+------------+----------------------------------+\n",
      "|POST        |POSTAGE                           |\n",
      "|DOT         |DOTCOM POSTAGE                    |\n",
      "|gift_0001_40|Dotcomgiftshop Gift Voucher �40.00|\n",
      "|C2          |CARRIAGE                          |\n",
      "|gift_0001_30|Dotcomgiftshop Gift Voucher �30.00|\n",
      "|BANK CHARGES|Bank Charges                      |\n",
      "|M           |Manual                            |\n",
      "|AMAZONFEE   |AMAZON FEE                        |\n",
      "|gift_0001_50|Dotcomgiftshop Gift Voucher �50.00|\n",
      "|gift_0001_20|Dotcomgiftshop Gift Voucher �20.00|\n",
      "|gift_0001_10|Dotcomgiftshop Gift Voucher �10.00|\n",
      "|S           |SAMPLES                           |\n",
      "|B           |Adjust bad debt                   |\n",
      "+------------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "excluded_stockcodes = [\"POST\", \"DOT\", \"M\", \"C2\", \"BANK CHARGES\",\"S\", \"B\", \"AMAZONFEE\",\n",
    "                       \"gift_0001_10\", \"gift_0001_20\",\"gift_0001_30\",\"gift_0001_40\",\"gift_0001_50\"]\n",
    "\n",
    "# Identify rows with exclued StockCode\n",
    "df_excluded = df_cleaned.filter(col(\"StockCode\").isin(excluded_stockcodes))\n",
    "\n",
    "# Show distinct excluded StockCode - Description pairs\n",
    "df_excluded.select(\"StockCode\", \"Description\").distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handle abnormal StockCode and Description pairs that are not actual products**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned.filter(~col(\"StockCode\").isin(excluded_stockcodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|StockCode|Description|\n",
      "+---------+-----------+\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Re-check the abnormal stock code\n",
    "df_excluded = df_cleaned.filter(col(\"StockCode\").isin(excluded_stockcodes))\n",
    "df_excluded.select(\"StockCode\", \"Description\").distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Data cleaning results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before cleaning: 541909\n"
     ]
    }
   ],
   "source": [
    "# The number of rows before cleaning\n",
    "rows_before_cleaning = df.count()\n",
    "print(f\"Number of rows before cleaning: {rows_before_cleaning}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after cleaning: 522541\n"
     ]
    }
   ],
   "source": [
    "# Check the number of rows after cleaning\n",
    "rows_after_cleaning = df_cleaned.count()\n",
    "print(f\"Number of rows after cleaning: {rows_after_cleaning}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Ensure Correct Data Types & Calculate TotalPrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure InvoiceDate is in timestamp format\n",
    "df_cleaned = df_cleaned.withColumn(\"InvoiceDate\", F.to_timestamp(F.col(\"InvoiceDate\"), \"M/d/yyyy H:mm\"))\n",
    "\n",
    "# Calculate the total price for each row\n",
    "df_cleaned = df_cleaned.withColumn(\"TotalPrice\", F.round(F.col(\"Quantity\") * F.col(\"UnitPrice\"), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of cleaned data with TotalPrice:\n",
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- TotalPrice: double (nullable = true)\n",
      "\n",
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+----------+\n",
      "|InvoiceNo|StockCode|Description                        |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |TotalPrice|\n",
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+----------+\n",
      "|536384   |84755    |COLOUR GLASS T-LIGHT HOLDER HANGING|48      |2010-12-01 09:53:00|0.65     |18074     |United Kingdom|31.2      |\n",
      "|536385   |22168    |ORGANISER WOOD ANTIQUE WHITE       |2       |2010-12-01 09:56:00|8.5      |17420     |United Kingdom|17.0      |\n",
      "|536399   |22632    |HAND WARMER RED POLKA DOT          |6       |2010-12-01 10:52:00|1.85     |17850     |United Kingdom|11.1      |\n",
      "|536401   |22767    |TRIPLE PHOTO FRAME CORNICE         |2       |2010-12-01 11:21:00|9.95     |15862     |United Kingdom|19.9      |\n",
      "|536423   |22632    |HAND WARMER RED RETROSPOT          |12      |2010-12-01 12:08:00|2.1      |18085     |United Kingdom|25.2      |\n",
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Schema of cleaned data with TotalPrice:\")\n",
    "df_cleaned.printSchema()\n",
    "df_cleaned.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Determine analysis date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Max Invoice Date: 2011-12-09 12:50:00\n",
      "Analysis Date: 2011-12-10 12:50:00\n"
     ]
    }
   ],
   "source": [
    "# The day after the last invoice date\n",
    "max_invoice_date = df_cleaned.agg(F.max(\"InvoiceDate\")).collect()[0][0]\n",
    "analysis_date = max_invoice_date + timedelta(days=1)\n",
    "analysis_date_lit = F.lit(analysis_date).cast(\"timestamp\")\n",
    "\n",
    "print(f\"\\nMax Invoice Date: {max_invoice_date}\")\n",
    "print(f\"Analysis Date: {analysis_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate RFM values per customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RFM Base Values:\n",
      "+----------+-------------------+---------+------------------+-------+\n",
      "|CustomerID|LastPurchaseDate   |Frequency|MonetaryValue     |Recency|\n",
      "+----------+-------------------+---------+------------------+-------+\n",
      "|15555     |2011-11-27 16:19:00|16       |4791.87           |13     |\n",
      "|15574     |2011-06-15 12:01:00|4        |675.64            |178    |\n",
      "|15634     |2011-11-22 16:28:00|1        |243.54999999999998|18     |\n",
      "|13610     |2011-11-27 15:38:00|7        |1082.33           |13     |\n",
      "|13192     |2011-09-05 14:13:00|2        |911.94            |96     |\n",
      "+----------+-------------------+---------+------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfm_df = df_cleaned.groupBy(\"CustomerID\").agg(\n",
    "    F.max(\"InvoiceDate\").alias(\"LastPurchaseDate\"),\n",
    "    F.countDistinct(\"InvoiceNo\").alias(\"Frequency\"),\n",
    "    F.sum(\"TotalPrice\").alias(\"MonetaryValue\")\n",
    ")\n",
    "\n",
    "# Calculate Recency in days\n",
    "rfm_df = rfm_df.withColumn(\n",
    "    \"Recency\",\n",
    "    F.datediff(analysis_date_lit, F.col(\"LastPurchaseDate\"))\n",
    ")\n",
    "print(\"\\nRFM Base Values:\")\n",
    "rfm_df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Create RFM Scores (using quartiles, so scores 1 - 4)\n",
    "RFM Scoring (1 to 4) — higher score = better customer\n",
    "\n",
    "We use NTILE(4) to split values into quartiles: rank 1 (lowest) to rank 4 (highest)\n",
    "\n",
    "Recency:\n",
    "- Fewer days = more recent = better\n",
    "- ntile gives rank 1 to lowest days → we want score 4 for that\n",
    "- So: R_Score = 5 - ntile_rank\n",
    "\n",
    "Frequency:\n",
    "- More orders = better\n",
    "- ntile gives rank 1 to lowest frequency → we want score 1 for that\n",
    "- So: F_Score = ntile_rank\n",
    "\n",
    "Monetary:\n",
    "- More money spent = better\n",
    "- ntile gives rank 1 to lowest spending → we want score 1 for that\n",
    "- So: M_Score = ntile_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+---------+-----------------+-------+----------+-------+\n",
      "|CustomerID|LastPurchaseDate   |Frequency|MonetaryValue    |Recency|R_Quartile|R_Score|\n",
      "+----------+-------------------+---------+-----------------+-------+----------+-------+\n",
      "|12680     |2011-12-09 12:50:00|4        |790.8100000000001|1      |1         |4      |\n",
      "|16705     |2011-12-09 12:08:00|20       |14034.99         |1      |1         |4      |\n",
      "|16626     |2011-12-09 11:56:00|17       |4413.1           |1      |1         |4      |\n",
      "|17364     |2011-12-09 09:00:00|11       |4462.68          |1      |1         |4      |\n",
      "|12518     |2011-12-09 10:13:00|5        |1840.89          |1      |1         |4      |\n",
      "+----------+-------------------+---------+-----------------+-------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quartiles for Recency (lower is better, so ascending order for ntile)\n",
    "r_window = Window.orderBy(F.col(\"Recency\").asc())\n",
    "rfm_df = rfm_df.withColumn(\"R_Quartile\", F.ntile(4).over(r_window))\n",
    "rfm_df = rfm_df.withColumn(\"R_Score\", (F.lit(5) - F.col(\"R_Quartile\")))\n",
    "rfm_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+---------+-------------+-------+----------+-------+----------+-------+\n",
      "|CustomerID|LastPurchaseDate   |Frequency|MonetaryValue|Recency|R_Quartile|R_Score|F_Quartile|F_Score|\n",
      "+----------+-------------------+---------+-------------+-------+----------+-------+----------+-------+\n",
      "|12713     |2011-12-09 12:16:00|1        |794.55       |1      |1         |4      |1         |1      |\n",
      "|13436     |2011-12-08 10:33:00|1        |196.89       |2      |1         |4      |1         |1      |\n",
      "|15520     |2011-12-08 10:58:00|1        |343.5        |2      |1         |4      |1         |1      |\n",
      "|13298     |2011-12-08 13:11:00|1        |360.0        |2      |1         |4      |1         |1      |\n",
      "|14569     |2011-12-08 14:58:00|1        |227.39       |2      |1         |4      |1         |1      |\n",
      "+----------+-------------------+---------+-------------+-------+----------+-------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quartiles for Frequency (higher is better, so ascending order for ntile)\n",
    "f_window = Window.orderBy(F.col(\"Frequency\").asc())\n",
    "rfm_df = rfm_df.withColumn(\"F_Quartile\", F.ntile(4).over(f_window))\n",
    "rfm_df = rfm_df.withColumn(\"F_Score\", F.col(\"F_Quartile\"))\n",
    "rfm_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quartiles for Monetary (higher is better, so ascending order for ntile)\n",
    "m_window = Window.orderBy(F.col(\"MonetaryValue\").asc())\n",
    "rfm_df = rfm_df.withColumn(\"M_Quartile\", F.ntile(4).over(m_window))\n",
    "rfm_df = rfm_df.withColumn(\"M_Score\", F.col(\"M_Quartile\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RFM with Scores:\n",
      "+----------+-------+---------+-------------+-------+-------+-------+\n",
      "|CustomerID|Recency|Frequency|MonetaryValue|R_Score|F_Score|M_Score|\n",
      "+----------+-------+---------+-------------+-------+-------+-------+\n",
      "|16738     |298    |1        |3.75         |1      |2      |1      |\n",
      "|16454     |65     |1        |5.9          |2      |1      |1      |\n",
      "|14792     |64     |1        |6.2          |2      |1      |1      |\n",
      "|17956     |250    |1        |12.75        |1      |2      |1      |\n",
      "|16878     |85     |1        |13.3         |2      |1      |1      |\n",
      "|13307     |121    |1        |15.0         |2      |1      |1      |\n",
      "|17763     |264    |1        |15.0         |1      |2      |1      |\n",
      "|16093     |107    |1        |17.0         |2      |1      |1      |\n",
      "|16953     |31     |1        |20.8         |3      |1      |1      |\n",
      "|17986     |57     |1        |20.8         |2      |1      |1      |\n",
      "|16257     |177    |1        |21.95        |1      |1      |1      |\n",
      "|18268     |135    |1        |25.5         |2      |1      |1      |\n",
      "|17102     |262    |1        |25.5         |1      |2      |1      |\n",
      "|15657     |23     |1        |30.0         |3      |1      |1      |\n",
      "|13120     |239    |1        |30.6         |1      |1      |1      |\n",
      "|17408     |164    |1        |32.65        |1      |1      |1      |\n",
      "|16765     |295    |1        |34.0         |1      |2      |1      |\n",
      "|15744     |78     |1        |34.8         |2      |1      |1      |\n",
      "|16856     |15     |1        |35.4         |4      |1      |1      |\n",
      "|17831     |34     |1        |35.4         |3      |1      |1      |\n",
      "+----------+-------+---------+-------------+-------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nRFM with Scores:\")\n",
    "rfm_df.select(\"CustomerID\", \"Recency\", \"Frequency\", \"MonetaryValue\", \"R_Score\", \"F_Score\", \"M_Score\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Combine RFM Scores into a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RFM with Concatenated Scores:\n",
      "+----------+-------+-------+-------+----------------+\n",
      "|CustomerID|R_Score|F_Score|M_Score|RFM_Score_Concat|\n",
      "+----------+-------+-------+-------+----------------+\n",
      "|16738     |1      |2      |1      |121             |\n",
      "|16454     |2      |1      |1      |211             |\n",
      "|14792     |2      |1      |1      |211             |\n",
      "|17956     |1      |2      |1      |121             |\n",
      "|16878     |2      |1      |1      |211             |\n",
      "+----------+-------+-------+-------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfm_final_df = rfm_df.withColumn(\n",
    "    \"RFM_Score_Concat\",\n",
    "    F.concat(F.col(\"R_Score\").cast(\"string\"), F.col(\"F_Score\").cast(\"string\"), F.col(\"M_Score\").cast(\"string\"))\n",
    ")\n",
    "\n",
    "print(\"\\nRFM with Concatenated Scores:\")\n",
    "rfm_final_df.select(\"CustomerID\", \"R_Score\", \"F_Score\", \"M_Score\", \"RFM_Score_Concat\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Define Customer Segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For scores 1-4, where 4 is best:\n",
    "- Champions: R=4, F=4, M=4 (Best Customers)\n",
    "- Loyal Customers: R >= 3, F >= 3, M >= 3 (but not Champions)\n",
    "- Potential Loyalists: Recent customers with good frequency or monetary value.\n",
    "- New Customers: High R (recent), low F, M.\n",
    "- Promising: Recent, but low F/M, or average R with high F/M.\n",
    "- Need Attention: Average R, F, M. Or R > 2, F < 3, M < 3.\n",
    "- At Risk: Purchased long ago, but were frequent/valuable. Or R < 3, F >=3, M >=3.\n",
    "- Can't Lose Them: Made big purchases, and were frequent, but haven't returned for a long time. (e.g., R=1, F=4, M=4)\n",
    "- Hibernating: Low F, M and low R (not recent).\n",
    "- Lost: Lowest R, F, M scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final RFM Segmentation:\n",
      "+----------+-------+---------+-------------+-------+-------+-------+----------------+----------------+\n",
      "|CustomerID|Recency|Frequency|MonetaryValue|R_Score|F_Score|M_Score|RFM_Score_Concat|Segment         |\n",
      "+----------+-------+---------+-------------+-------+-------+-------+----------------+----------------+\n",
      "|16738     |298    |1        |3.75         |1      |2      |1      |121             |Hibernating/Lost|\n",
      "|16454     |65     |1        |5.9          |2      |1      |1      |211             |Hibernating/Lost|\n",
      "|14792     |64     |1        |6.2          |2      |1      |1      |211             |Hibernating/Lost|\n",
      "|17956     |250    |1        |12.75        |1      |2      |1      |121             |Hibernating/Lost|\n",
      "|16878     |85     |1        |13.3         |2      |1      |1      |211             |Hibernating/Lost|\n",
      "|13307     |121    |1        |15.0         |2      |1      |1      |211             |Hibernating/Lost|\n",
      "|17763     |264    |1        |15.0         |1      |2      |1      |121             |Hibernating/Lost|\n",
      "|16093     |107    |1        |17.0         |2      |1      |1      |211             |Hibernating/Lost|\n",
      "|16953     |31     |1        |20.8         |3      |1      |1      |311             |Promising       |\n",
      "|17986     |57     |1        |20.8         |2      |1      |1      |211             |Hibernating/Lost|\n",
      "|16257     |177    |1        |21.95        |1      |1      |1      |111             |Hibernating/Lost|\n",
      "|18268     |135    |1        |25.5         |2      |1      |1      |211             |Hibernating/Lost|\n",
      "|17102     |262    |1        |25.5         |1      |2      |1      |121             |Hibernating/Lost|\n",
      "|15657     |23     |1        |30.0         |3      |1      |1      |311             |Promising       |\n",
      "|13120     |239    |1        |30.6         |1      |1      |1      |111             |Hibernating/Lost|\n",
      "|17408     |164    |1        |32.65        |1      |1      |1      |111             |Hibernating/Lost|\n",
      "|16765     |295    |1        |34.0         |1      |2      |1      |121             |Hibernating/Lost|\n",
      "|15744     |78     |1        |34.8         |2      |1      |1      |211             |Hibernating/Lost|\n",
      "|16856     |15     |1        |35.4         |4      |1      |1      |411             |New Customers   |\n",
      "|17831     |34     |1        |35.4         |3      |1      |1      |311             |Promising       |\n",
      "+----------+-------+---------+-------------+-------+-------+-------+----------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "segment_col = (\n",
    "    F.when((F.col(\"R_Score\") == 4) & (F.col(\"F_Score\") == 4) & (F.col(\"M_Score\") == 4), \"Champions\")\n",
    "    .when((F.col(\"R_Score\") >= 3) & (F.col(\"F_Score\") >= 3) & (F.col(\"M_Score\") >= 3), \"Loyal Customers\") # Excludes Champions\n",
    "    .when((F.col(\"R_Score\") == 4) & (F.col(\"F_Score\") >= 2) & (F.col(\"M_Score\") >= 2), \"Potential Loyalists\") # Recent, decent F/M\n",
    "    .when((F.col(\"R_Score\") == 4) & (F.col(\"F_Score\") == 1) & (F.col(\"M_Score\") == 1), \"New Customers\")\n",
    "    .when((F.col(\"R_Score\") == 1) & (F.col(\"F_Score\") >= 3) & (F.col(\"M_Score\") >= 3), \"Season customer\") # High F/M but not recent\n",
    "    .when((F.col(\"R_Score\") <= 2) & (F.col(\"F_Score\") >= 2) & (F.col(\"M_Score\") >= 2), \"At Risk\") # Not recent, but were good\n",
    "    .when((F.col(\"R_Score\") >= 3) & (F.col(\"F_Score\") <= 2) & (F.col(\"M_Score\") <= 2), \"Promising\") # Recent, but low F/M\n",
    "    .when((F.col(\"R_Score\") <= 2) & (F.col(\"F_Score\") <= 2) & (F.col(\"M_Score\") <= 2), \"Hibernating/Lost\")\n",
    "    .otherwise(\"Need Attention\") # General catch-all for average customers\n",
    ")\n",
    "\n",
    "rfm_final_df = rfm_final_df.withColumn(\"Segment\", segment_col)\n",
    "print(\"\\nFinal RFM Segmentation:\")\n",
    "rfm_final_df.select(\"CustomerID\", \"Recency\", \"Frequency\", \"MonetaryValue\", \"R_Score\", \"F_Score\", \"M_Score\", \"RFM_Score_Concat\", \"Segment\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Customer Count per Segment:\n",
      "+-------------------+-----+\n",
      "|            Segment|count|\n",
      "+-------------------+-----+\n",
      "|   Hibernating/Lost|  945|\n",
      "|            At Risk|  928|\n",
      "|    Loyal Customers|  808|\n",
      "|          Champions|  489|\n",
      "|          Promising|  453|\n",
      "|     Need Attention|  351|\n",
      "|Potential Loyalists|  172|\n",
      "|    Season customer|  130|\n",
      "|      New Customers|   59|\n",
      "+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyze the distribution of customers across segments\n",
    "print(\"\\nCustomer Count per Segment:\")\n",
    "rfm_final_df.groupBy(\"Segment\").count().orderBy(F.desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average RFM Values per Segment:\n",
      "+-------------------+-----------+-------------+------------+\n",
      "|Segment            |Avg_Recency|Avg_Frequency|Avg_Monetary|\n",
      "+-------------------+-----------+-------------+------------+\n",
      "|Champions          |7.8        |18.2         |12059.41    |\n",
      "|Loyal Customers    |23.2       |5.8          |2432.3      |\n",
      "|Season customer    |219.5      |3.8          |2152.72     |\n",
      "|Potential Loyalists|9.5        |2.5          |1642.62     |\n",
      "|At Risk            |142.4      |3.1          |1232.48     |\n",
      "|Need Attention     |87.3       |2.1          |872.73      |\n",
      "|Promising          |29.3       |1.4          |316.42      |\n",
      "|Hibernating/Lost   |184.8      |1.1          |231.75      |\n",
      "|New Customers      |10.3       |1.0          |170.77      |\n",
      "+-------------------+-----------+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average R, F, M per segment\n",
    "print(\"\\nAverage RFM Values per Segment:\")\n",
    "rfm_final_df.groupBy(\"Segment\").agg(\n",
    "    F.round(F.avg(\"Recency\"), 1).alias(\"Avg_Recency\"),\n",
    "    F.round(F.avg(\"Frequency\"), 1).alias(\"Avg_Frequency\"),\n",
    "    F.round(F.avg(\"MonetaryValue\"), 2).alias(\"Avg_Monetary\")\n",
    ").orderBy(F.desc(\"Avg_Monetary\")).show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
